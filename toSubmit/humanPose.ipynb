{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "humanPose.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rObFwZmbHoF"
      },
      "source": [
        "## if true, will only run on one sample that is provided with code for grading purposes \n",
        "runonsample = True\n",
        "\n",
        "## run on dataset is dependent on all data being available in the specified path\n",
        "runondataset = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kU_e8YObKZV"
      },
      "source": [
        "if runondataset:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  rootPath = '/content/drive/My Drive/DL_CV_FinalProject/'\n",
        "  #dataPath = '/content/data/'\n",
        "  ourdata_path = rootPath + 'Datasets/our_fall_dataset/'\n",
        "  urfd_path = rootPath + 'URFD_Videos/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggpthOSdBFWy"
      },
      "source": [
        "# install dependencies: \n",
        "!pip install pyyaml==5.1\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab\n",
        "assert torch.__version__.startswith(\"1.7\")\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-6FOICnBroB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9045f4-aefc-4053-d0fc-bb3829680e1c"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "#from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import pandas as pd \n",
        "import glob"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTlcHqkFh6p4",
        "outputId": "b375cd36-c7cf-4a48-8d60-1c5eb7a50f94"
      },
      "source": [
        "## Check for GPU \n",
        "gpu_check = torch.cuda.is_available()\n",
        "print(gpu_check)\n",
        "\n",
        "if runondataset:\n",
        "  ourDataset = sorted(glob.glob(ourdata_path + '*'))\n",
        "  urfdDataset = sorted(glob.glob(urfd_path + '*'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qA-ZqeHNw9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06e6f54-d6d3-4caf-9c1c-614affdc8f36"
      },
      "source": [
        "## Initialize class for predictor functions\n",
        "class ReturnPredictions(object):\n",
        "    def __init__(self,cfg):\n",
        "        self.predictor = DefaultPredictor(cfg)\n",
        "        self.metadata = MetadataCatalog.get(\n",
        "                cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\")\n",
        "\n",
        "    def _frame_from_video(self,video):\n",
        "        print(video)\n",
        "        while video.isOpened():\n",
        "            success,frame = video.read()\n",
        "            if success:\n",
        "                yield frame\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def run_on_video(self,video):\n",
        "        frame_gen = self._frame_from_video(video)\n",
        "        for frame in frame_gen:\n",
        "            \n",
        "            yield self.predictor(frame),frame\n",
        "\n",
        "    def run_on_image(self,image):\n",
        "      # Convert image from OpenCV BGR format to Matplotlib RGB format.\n",
        "      yield self.predictor(image),image\n",
        "\n",
        "\n",
        "## Load and setup model and predictor\n",
        "cfg = get_cfg()\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "predictor = ReturnPredictions(cfg)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_final_a6e10b.pkl: 237MB [00:23, 10.1MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s849oNYt6EJ1"
      },
      "source": [
        "def getAvgVel(keypts_allFrames,kp_thresh = 0.09):\n",
        "  numFrames,_,_ = keypts_allFrames.shape\n",
        "   \n",
        "  keypoint_dict = {\n",
        "      \"nose\":[],\n",
        "      \"left_eye\":[], \"right_eye\":[],\n",
        "      \"left_ear\":[], \"right_ear\":[],\n",
        "      \"left_shoulder\":[], \"right_shoulder\":[],\n",
        "      \"left_elbow\":[], \"right_elbow\":[],\n",
        "      \"left_wrist\":[], \"right_wrist\":[],\n",
        "      \"left_hip\":[], \"right_hip\":[],\n",
        "      \"left_knee\":[], \"right_knee\":[],\n",
        "      \"left_ankle\":[], \"right_ankle\":[],\n",
        "  }\n",
        "\n",
        "  keypointNames_MasterList = list(keypoint_dict)\n",
        "\n",
        "  keypoint_dict_diff = {\n",
        "      \"nose\":[None],\n",
        "      \"left_eye\":[None], \"right_eye\":[None],\n",
        "      \"left_ear\":[None], \"right_ear\":[None],\n",
        "      \"left_shoulder\":[None], \"right_shoulder\":[None],\n",
        "      \"left_elbow\":[None], \"right_elbow\":[None],\n",
        "      \"left_wrist\":[None], \"right_wrist\":[None],\n",
        "      \"left_hip\":[None], \"right_hip\":[None],\n",
        "      \"left_knee\":[None], \"right_knee\":[None],\n",
        "      \"left_ankle\":[None], \"right_ankle\":[None],\n",
        "  }\n",
        "\n",
        "  for i,framePts in enumerate(keypts_allFrames):\n",
        "    for j,kp in enumerate(framePts):\n",
        "      kpName = keypointNames_MasterList[j]\n",
        "      if kp[2] > kp_thresh:\n",
        "        keypoint_dict[kpName].append(kp[:2])\n",
        "      else:\n",
        "        keypoint_dict[kpName].append(np.array([None]))\n",
        "\n",
        "  for kpName in keypoint_dict:\n",
        "    pts = keypoint_dict[kpName]\n",
        "    for i in range(1,len(pts)):\n",
        "      if (pts[i-1].any() != None) and (pts[i].any() != None):\n",
        "        keypoint_dict_diff[kpName].append(getMag(pts[i]-pts[i-1]))\n",
        "      else:\n",
        "        keypoint_dict_diff[kpName].append(None)\n",
        "\n",
        "  frameAvgDiff = [0]\n",
        "  for i in range(1,numFrames):\n",
        "    tmpDiffList_ = []\n",
        "    for bodyPt in keypoint_dict_diff:\n",
        "      tmpDiffList_.append(keypoint_dict_diff[bodyPt][i])\n",
        "\n",
        "    tmpDiffList = np.array([v for v in tmpDiffList_ if v != None])\n",
        "\n",
        "    if len(tmpDiffList) == 0:\n",
        "      frameAvgDiff.append(None)\n",
        "    else:\n",
        "      diffMean = np.mean(tmpDiffList)\n",
        "      diffSTD = np.std(tmpDiffList)\n",
        "      distance_from_mean = abs(tmpDiffList - diffMean)\n",
        "      max_deviations = 1\n",
        "      not_outlier = tmpDiffList[distance_from_mean < max_deviations * diffSTD]\n",
        "      if len(not_outlier) == 0:\n",
        "        frameAvgDiff.append(None)\n",
        "      else:\n",
        "        frameAvgDiff.append(np.mean(not_outlier))\n",
        "\n",
        "  return frameAvgDiff\n",
        "\n",
        "def takeManualDeriv(signal,t):\n",
        "  out_sig = []\n",
        "\n",
        "  for i in range(1,len(signal)):\n",
        "    if (signal[i]==None) or (signal[i-1]==None):\n",
        "      out_sig.append(None)\n",
        "    else:\n",
        "      out_sig.append(signal[i]-signal[i-1])\n",
        "  return out_sig,t[1:]\n",
        "\n",
        "def scaleArea(orig_frame, targetArea=(512*288)):\n",
        "  frameArea = orig_frame.shape[0]*orig_frame.shape[1]\n",
        "  s = np.sqrt(targetArea/frameArea)\n",
        "  dim = (int(orig_frame.shape[1]*s),int(orig_frame.shape[0]*s))\n",
        "  resized = cv2.resize(orig_frame, dim, interpolation = cv2.INTER_AREA)\n",
        "  return resized,s\n",
        "\n",
        "def resizeKPB(keyPoints,bbox, scale):\n",
        "  # rescale keypoints and bounding boxes to match new image dimensions \n",
        "  keyPoints[:,:-1] = keyPoints[:,:-1]*scale\n",
        "  bbox = bbox*scale\n",
        "  return keyPoints,bbox\n",
        "\n",
        "def getMag(pt_diff):\n",
        "  return np.sqrt(pt_diff[0]**2 + pt_diff[1]**2)\n",
        "\n",
        "def movAvg(signal,time_v,winSize=5):\n",
        "    avgSig = []\n",
        "    for i in range(np.floor(winSize/2).astype(int),len(signal)-np.floor(winSize/2).astype(int)):\n",
        "        start_id = i-np.floor(winSize/2).astype(int)\n",
        "        end_id = i+np.ceil(winSize/2).astype(int)\n",
        "        subSig = [v for v in signal[start_id:end_id] if v != None] \n",
        "\n",
        "        if len(subSig) != 0: \n",
        "          avgSig.append(np.mean(subSig))\n",
        "        else:\n",
        "          avgSig.append(None)\n",
        "        \n",
        "    t = time_v[np.floor(winSize/2).astype(int):np.floor(winSize/2).astype(int)+len(avgSig)]\n",
        "\n",
        "    return t,avgSig"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXU7DejOCSkS"
      },
      "source": [
        "def run_detector_video(videoPath,vis=False):\n",
        "  storeKeypoints = [] \n",
        "  storeFrames = [] # store grayscale frames\n",
        "\n",
        "  # arrays of 0s to append when no person present \n",
        "  noPointsArray = np.zeros((17,3))\n",
        "  personThresh = 0.8\n",
        "\n",
        "  # create detectron points and bounding box generator \n",
        "  vid = cv2.VideoCapture(videoPath)\n",
        "  output_generator = predictor.run_on_video(vid) \n",
        "\n",
        "  imgID = 0\n",
        "\n",
        "  # iterate through frames \n",
        "  for output_frame in output_generator:\n",
        "\n",
        "    if vis:\n",
        "      print('---------------- Image Number: ',imgID)\n",
        "\n",
        "    output = output_frame[0]\n",
        "    frame = output_frame[1]\n",
        "\n",
        "    # scale image down \n",
        "    g_frame,_ = scaleArea(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
        "    \n",
        "    storeFrames.append(g_frame)\n",
        "\n",
        "    # extract values from output \n",
        "    keypoints_all = output['instances'].pred_keypoints.cpu().numpy()\n",
        "    person_scores = output['instances'].scores.cpu().numpy()\n",
        "    bound_box = output['instances'].pred_boxes.tensor.cpu().numpy()\n",
        "\n",
        "    # extract indices of cases where person is detected \n",
        "    idx_scores_valid = np.argwhere(person_scores>personThresh)\n",
        "    num_valid = len(idx_scores_valid)\n",
        "\n",
        "    imgID += 1\n",
        "\n",
        "    if len(idx_scores_valid) == 0:\n",
        "      # if no person detected \n",
        "\n",
        "      if vis:\n",
        "        print('Could not detect person!')\n",
        "      storeKeypoints.append(noPointsArray)\n",
        "\n",
        "    else:    \n",
        "      if vis:\n",
        "        print(\"All Scores:\",person_scores)\n",
        "\n",
        "      # resize RGB image for showing figure \n",
        "      imgCopy,scale_factor = scaleArea(cv2.cvtColor(frame.copy(),cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      # resize kps and bboxes \n",
        "      key,box = resizeKPB(keypoints_all[np.argmax(person_scores)],\n",
        "                          bound_box[np.argmax(person_scores)], \n",
        "                          scale_factor)\n",
        "\n",
        "      storeKeypoints.append(key)\n",
        "\n",
        "      #draw bounding boxes and key points \n",
        "      if vis:\n",
        "        print('---height,width',(abs(box[3] - box[1]),abs(box[2] - box[0])))\n",
        "        cv2.rectangle(imgCopy, (box[0],box[1]), (box[2],box[3]), (255,0,0),2)\n",
        "        for pt in key:\n",
        "          #print('Confidence = ',pt[2])\n",
        "          if pt[2] > 0.1:\n",
        "            cv2.circle(imgCopy, (pt[0],pt[1]), 2, (0,255,0), -1)\n",
        "        plt.imshow(imgCopy)\n",
        "        plt.show()\n",
        "\n",
        "  storeKeypoints_ = np.stack(storeKeypoints,axis=0)\n",
        "  storeFrames_ = np.stack(storeFrames,axis=0)\n",
        "\n",
        "  return storeKeypoints_"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt8I6Tt99tcr"
      },
      "source": [
        "def getClass(kps,classThresh=3.5):\n",
        "  numFrames = len(kps)\n",
        "  avgVel = getAvgVel(kps,kp_thresh = 0.1)\n",
        "  tvel,vel = movAvg(avgVel,np.arange(numFrames),winSize=5)\n",
        "  acc,tacc = takeManualDeriv(vel,tvel)\n",
        "\n",
        "  acc_nozero = [v for v in acc if v != None]\n",
        "  print('acc max',np.max(acc_nozero))\n",
        "  print('acc min',np.min(acc_nozero))\n",
        "\n",
        "  f_pred = 0\n",
        "  start_idx = None\n",
        "  end_idx = None\n",
        "  for idx,a in enumerate(acc):\n",
        "    if a != None:\n",
        "      # if a > the threshold and onset has not yet been detected\n",
        "      if (a > classThresh) and (f_pred==0):\n",
        "        f_pred=1\n",
        "        start_idx = idx\n",
        "      \n",
        "      # if an onset has been predicted a goes below the -thresh then predict an ending \n",
        "      if (f_pred==1) and (a < -classThresh):\n",
        "        end_idx = idx\n",
        "        return f_pred,start_idx,end_idx\n",
        "  \n",
        "  if (start_idx == None) and (end_idx == None):\n",
        "    return 0,0,0\n",
        "  elif (start_idx != None) and (end_idx == None):\n",
        "    end_idx = numFrames\n",
        "    return f_pred, start_idx, end_idx"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6GyjKCXK2Wp"
      },
      "source": [
        "# RUN ON OUR DATASET AND SAVE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrRpstprQlwS"
      },
      "source": [
        "if runondataset:\n",
        "  outMaster_fname = rootPath + 'results/our_dataset_master.csv'\n",
        "  saveData_fname = rootPath + 'results/our_dataset_dec17_HUMANPOSEONLY.csv'\n",
        "  outMaster = pd.read_csv(outMaster_fname)\n",
        "\n",
        "  for sampleName in ourDataset:\n",
        "    sampleID = sampleName.split('/')[-1].split('.')[0]\n",
        "    print('-----Running Sample:', sampleID)\n",
        "    print('Fall Class = ', outMaster[outMaster.sequence_name==sampleID]['fall_label'] )\n",
        "    keyPointsAllFrames= run_detector_video(sampleName)\n",
        "    fallPred,fallStart,fallEnd = getClass(keyPointsAllFrames,classThresh=1.5)\n",
        "    print('Fall Prediction = ', fallPred)\n",
        "    outMaster.loc[outMaster.sequence_name==sampleID,'fall_pred'] = fallPred\n",
        "    outMaster.loc[outMaster.sequence_name==sampleID,'fall_start_pred'] = fallStart\n",
        "    outMaster.loc[outMaster.sequence_name==sampleID,'fall_end_pred'] = fallEnd\n",
        "    #outMaster.to_csv(saveData_fname)\n",
        "\n",
        "  outMaster.to_csv(saveData_fname)   "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggNA2AYUL443"
      },
      "source": [
        "# RUN ON URFD DATASET AND SAVE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLC-F5JvF75K"
      },
      "source": [
        "if runondataset:\n",
        "  outMaster_fname = rootPath + 'results/urfd_master.csv'\n",
        "  saveData_fname = rootPath + 'results/urfd_output.csv'\n",
        "  outMaster = pd.read_csv(outMaster_fname)\n",
        "\n",
        "  for sampleName in urfdDataset:\n",
        "    sampleID = sampleName.split('/')[-1][:-9]\n",
        "    print('-----Running Sample:', sampleID)\n",
        "    print('Fall Class = ', outMaster[outMaster.sequence_name==sampleID]['fall_label'] )\n",
        "    keyPointsAllFrames= run_detector_video(sampleName)\n",
        "    fallPred,fallStart,fallEnd = getClass(keyPointsAllFrames,classThresh=1.5)\n",
        "    print('Fall Prediction = ', fallPred)\n",
        "    outMaster.loc[outMaster.sequence_name==sampleID,'fall_pred'] = fallPred\n",
        "    outMaster.loc[outMaster.sequence_name==sampleID,'fall_start_pred'] = fallStart\n",
        "    outMaster.loc[outMaster.sequence_name==sampleID,'fall_end_pred'] = fallEnd\n",
        "    #outMaster.to_csv(saveData_fname)\n",
        "\n",
        "  outMaster.to_csv(saveData_fname) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMmA3P4j1YbT"
      },
      "source": [
        "Please upload the following files into your colab runtime if running on colab (they are located within the submitted files):\n",
        "*   our_dataset_master.csv\n",
        "*   erica-11.mp4\n",
        "\n",
        "If running locally, structure has been implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTzE-RqN56yq"
      },
      "source": [
        "colab = False\n",
        "local = True\n",
        "ViewSetting = False ## Change view setting if you would like to see OF and Keypoints\n",
        "if runonsample:\n",
        "  if colab:\n",
        "    annotationsFile = 'our_dataset_master.csv'\n",
        "    sampleName = 'erica-11.mp4'\n",
        "    outMaster = pd.read_csv(annotationsFile)\n",
        "    sampleID = sampleName.split('/')[-1].split('.')[0]\n",
        "    print('-----Running Sample:', sampleID)\n",
        "    print('Fall Label = ', outMaster[outMaster.sequence_name==sampleID]['fall_label'].item() )\n",
        "    print('Start Frame Ground Truth = ', outMaster[outMaster.sequence_name==sampleID]['fall_start_frame'].item() )\n",
        "    print('End Frame Ground Truth = ', outMaster[outMaster.sequence_name==sampleID]['fall_end_frame'].item() )\n",
        "    keyPointsAllFrames= run_detector_video(sampleName)\n",
        "    fallPred,fallStart,fallEnd = getClass(keyPointsAllFrames,classThresh=1.5,vis=ViewSetting)\n",
        "    print('Fall Predictions = ', fallPred)\n",
        "    print('Start Frame Detected = ', fallStart)\n",
        "    print('End Frame Detected = ',fallEnd)\n",
        "\n",
        "  if local:\n",
        "    annotationsFile = './our_dataset_master.csv'\n",
        "    sampleName = './erica-11.mp4'\n",
        "    outMaster = pd.read_csv(annotationsFile)\n",
        "    sampleID = sampleName.split('/')[-1].split('.')[0]\n",
        "    print('-----Running Sample:', sampleID)\n",
        "    print('Fall Label = ', outMaster[outMaster.sequence_name==sampleID]['fall_label'].item() )\n",
        "    print('Start Frame Ground Truth = ', outMaster[outMaster.sequence_name==sampleID]['fall_start_frame'].item() )\n",
        "    print('End Frame Ground Truth = ', outMaster[outMaster.sequence_name==sampleID]['fall_end_frame'].item() )\n",
        "    keyPointsAllFrames= run_detector_video(sampleName)\n",
        "    fallPred,fallStart,fallEnd = getClass(keyPointsAllFrames,classThresh=1.5)\n",
        "    print('Fall Predictions = ', fallPred)\n",
        "    print('Start Frame Detected = ', fallStart)\n",
        "    print('End Frame Detected = ',fallEnd)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmtfMFfI2YDN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}